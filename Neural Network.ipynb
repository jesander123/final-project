{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8787abcb-96b6-4330-90a2-11e4eafd8027",
   "metadata": {},
   "source": [
    "# Network Model 2\n",
    "This visualization is meant to mimic a human neural network. This model consists of three layers: input layer, association layer, and output layer. The input layer intakes the raw feature data such as loudness, pitch, timbre from the audio. The association layer are the intermediate chromesthesia associations between input features and visual features - mostly working to combine multiple audio features to then activate a visual image associated with those features. Output layer nodes are the visual layer in which activation determines what shapes, color , intnesity the user sees. As the visual output begins, all nodes in adjacent layers are automatically connected to each other (input nodes all connected to association, association nodes all connected to output) with randomized weights set, but as the audio plays the weights have the opportunity to be strengthened based on the learning mechanisms in play. When the audio plays, the input nodes are set, they are turned on in proportion to its strength in the audio. They now contain a signal which is then transmitted to following layers through the edges, similating how action potentials are propagated through neurons carrying a signal. If both the start and end node are activated at the same time, the edge weight increases (Hebbian learning- \"nodes that fire together, wire together), hence why edges grow thicker\n",
    "\n",
    "Sources:\n",
    "- https://www.jeremykun.com/2012/12/09/neural-networks-and-backpropagation/?utm_source=chatgpt.com- setting up nodes and edges\n",
    "- https://github.com/SophieWalden/snakeNeuralNetwork/blob/master/snakeNN.py- neural network example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0826b73f-4eb8-4dde-826c-0b8884a1bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411029d4-35b6-4ce8-9418-4d08a2e94468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pygame\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea9d846-7efd-4fca-9b9e-3740ef22623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented same way as from feature extraction doc\n",
    "\n",
    "# predefined colors for each note (implement user entry later)\n",
    "NOTE_RGB = {\n",
    "    \"C\":  (214, 174, 16),\n",
    "    \"C#\": (115, 58, 75),\n",
    "    \"D\":  (38, 63, 145),\n",
    "    \"D#\": (68, 118, 67),\n",
    "    \"E\":  (211, 87, 49),\n",
    "    \"F\":  (159, 194, 76),\n",
    "    \"F#\": (195, 10, 103),\n",
    "    \"G\":  (255, 156, 223),\n",
    "    \"G#\": (37, 149, 150),\n",
    "    \"A\":  (155, 152, 223),\n",
    "    \"A#\": (10, 107, 62),\n",
    "    \"B\":  (238, 145, 50)\n",
    "}\n",
    "NOTE_NAMES = [\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"]\n",
    "\n",
    "# predefined cluster shapes \n",
    "CLUSTER_SHAPES = {\n",
    "    0: \"diamond\", \n",
    "    1: \"circle\",    \n",
    "    2: \"wave\",        \n",
    "    }\n",
    "\n",
    "def extract_audio_features(audio_path: str, duration: float = None, HOP_LENGTH: int = 2048, FRAME_LENGTH: int = 2048 ):\n",
    "    \"\"\"\n",
    "    This function loads an audio file and extracts audio features: pitch (f0), loudness (RMS energy), and timbre (MFCC) for each beat frame.\n",
    "    It also normalizes rms and MFCC on a 0-1 scale for visual mapping.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path: str\n",
    "            path to any audio file format (.wav, .mp3, .ogg, etc.)\n",
    "        duration: float, optional\n",
    "            Duration from the start of the file (seconds). \n",
    "            Loads the full track if none. \n",
    "        hop_length: int, fixed\n",
    "            Number of samples between frames.\n",
    "        frame_length: int, fixed\n",
    "            Number of samples per frame. \n",
    "\n",
    "    Returns:\n",
    "         audio_features : dict\n",
    "            times: array of frame timestamps,\n",
    "            midi: midi number ,\n",
    "            rms: normalized loudness,\n",
    "            mfcc: normalized MFCC matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # load audio file\n",
    "    y, sr = librosa.load(audio_path, duration=duration)\n",
    "\n",
    "    # 1. Pitch (f0) extraction\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y,\n",
    "                                             sr=sr,\n",
    "                                             frame_length = FRAME_LENGTH,\n",
    "                                             hop_length = HOP_LENGTH,\n",
    "                                             fmin=librosa.note_to_hz('A0'),\n",
    "                                             fmax=librosa.note_to_hz('C8'))\n",
    "    f0 = np.nan_to_num(f0, nan=np.nanmean(f0))  \n",
    "    # if there are NaN values, replace them with the mean pitch\n",
    "    midi = librosa.hz_to_midi(f0)\n",
    "    # convert frequency to midi note number\n",
    "\n",
    "    # 2. Loudness(RMS energy) extraction\n",
    "    rms = librosa.feature.rms(y=y,frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n",
    "    \n",
    "    # 3. Timbre (MFCC) extraction\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # time stamps for mapping to pygame \n",
    "    times = librosa.times_like(rms, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # normalize numerical features for mapping (0â€“1 range)\n",
    "    def normalize(x):\n",
    "        min = np.min(x)\n",
    "        max = np.max(x)\n",
    "        denom = max - min\n",
    "        if denom == 0:\n",
    "            return np.zeros_like(x)  \n",
    "        return (x - min) / denom\n",
    "    \n",
    "    rms_norm = normalize(rms)\n",
    "    mfcc_norm = np.apply_along_axis(normalize, 1, mfcc)\n",
    "    \n",
    "    # K-Means Clustering on MFCC to classify timbre into 3 general groups\n",
    "    mfcc_T = mfcc_norm.T   # take transpose of MFCC so each row is a time stamp and cols are the 13 features\n",
    "    scaler = StandardScaler()  \n",
    "    mfcc_scaled = scaler.fit_transform(mfcc_T) # standardize each coefficent\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(mfcc_scaled)\n",
    "\n",
    "    # dictionary of audio features for the funciton to return\n",
    "    audio_features = {\n",
    "       \"times\" : times,\n",
    "       \"midi\" : midi,\n",
    "       \"rms\" : rms_norm,\n",
    "       \"mfcc\" : mfcc_norm,\n",
    "       \"cluster_labels\" : cluster_labels,\n",
    "       \"sr\" : sr \n",
    "       }\n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c64b401-1b38-4643-9f14-046b38db626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # creates class for nodes\n",
    "    def __init__(self, x, y, node_type=\"input\", feature=None):\n",
    "        # init function, takes in self, x, y (referring to coordinate positions in x and y planes), what layer the node is a part of\n",
    "        self.x = x\n",
    "        # node's x position\n",
    "        self.y = y\n",
    "        # refers to node's y position\n",
    "        self.type = node_type\n",
    "        # refers to what layer the node is a part of- this neural network will have three layers (input, association, output/visual), helps for display\n",
    "        self.feature = feature\n",
    "        # pitch, rms, timbre --> what each node will represent\n",
    "        self.activation = 0.0\n",
    "        # current activation level for node\n",
    "        self.pulse_size = 1.0\n",
    "        # how much the node will pulse\n",
    "        self.edges = []\n",
    "        # initialize empty list to hold edges\n",
    "    def add_edge(self, target_node, weight= 0.1):\n",
    "        self.edges.append(Edge(self,target_node,weight))\n",
    "        # actually able to add the edge between noe\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, start_node, end_node, weight= 0.1):\n",
    "        \"\"\"\n",
    "        sends activation from starting node to target node. \n",
    "        \"\"\"\n",
    "        # this function describes how to add edges\n",
    "        self.start_node = start_node\n",
    "        # sets start node as where signal begins\n",
    "        self.end_node = end_node\n",
    "        # sets end node, where edge will be connected to/end\n",
    "        self.weight = weight\n",
    "        # strength of edge/connection between nodes with a higher value meaning more connection (this weight will be modified)\n",
    "    def propagate(self):\n",
    "        signal = self.start_node.activation * self.weight\n",
    "        # self.from_node.activation = node we are starting at, activation is coming from audio and its extracted features\n",
    "        # weight refers to strength of connective edges between nodes\n",
    "        # multiplying them together to get the size of signal - if activation and weight are both high, then there's a big signal\n",
    "        # if weight is low, then there's a weak signal, but if the node isn't activated at all (or weak) then no signal sent (or just super weak)\n",
    "        self.end_node.activation = self.end_node.activation + signal\n",
    "        # adds the already existent activity in the ending node of the edge and adds the signal propagating through to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4111d192-7f47-4497-a032-645292e1081a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class NeuralVisualizer:\n",
    "    \"\"\"\n",
    "    Create neural visualizer class that will create a visualization of the neural network output. \n",
    "    Consists of three layers, input, association, output, each with separate nodes. Meant to simulate a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self,audio_path,features,height=800,width=1000, fps=60, learning_rate=0.01):\n",
    "        # init function, takes in audio path to connect to audio, features as created above (shapes, colors, etc), size of image, frames per second, and learning rate\n",
    "        pygame.init()\n",
    "        # initialize all pygame parts\n",
    "        pygame.mixer.init()\n",
    "        # initialize audio playback\n",
    "        pygame.display.set_caption(\"Chromesthesia Neural Visualizer\")\n",
    "        # captions with informative title\n",
    "        self.clock = pygame.time.Clock()\n",
    "        # control frame rate and time\n",
    "        self.learning_rate = learning_rate\n",
    "        # used for learnign rate of node associations - will change over time\n",
    "        self.width = width \n",
    "        # set width display\n",
    "        self.height = height\n",
    "        # set height display\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        # this creates the display/window for output and sets it to width/height set above\n",
    "        self.fps = fps\n",
    "        # frames per second- controls how often this is updated in the window\n",
    "\n",
    "        self.start_time = 0\n",
    "        # when playback starts for audio and visualization\n",
    "        self.current_frame = 0\n",
    "        # tracks frames of visualization, intialize to zero to start\n",
    "        self.running = False\n",
    "        # don't start running yet\n",
    "\n",
    "        self.audio_path = audio_path\n",
    "        # connects to audio\n",
    "        self.times = features[\"times\"]\n",
    "        # timestamps for each frame\n",
    "        self.midi = features[\"midi\"]\n",
    "        # takes midi notes- pitch in midi numbers associated with color\n",
    "        self.rms = features[\"rms\"]\n",
    "        # loudness- used for scaling\n",
    "        self.mfcc = features[\"mfcc\"]\n",
    "        # timbre coefficients\n",
    "        self.sr = features[\"sr\"]\n",
    "        # sample rate of audio\n",
    "        self.cluster_labels = features[\"cluster_labels\"]\n",
    "        # timbre clusters \n",
    "\n",
    "        self.input_nodes = []\n",
    "        # input layer nodes- represents raw features of audio- pitch, timbre, loudness\n",
    "        self.association_nodes = []\n",
    "        # association/middle layer of nodes- create the chromestesia associations- color, shape\n",
    "        self.output_nodes = []\n",
    "        # output/visualization layer- visual representation of nodes\n",
    "        self.max_radius = 20\n",
    "        # just sets a maximum size a node can grow to\n",
    "\n",
    "        \n",
    "        n_inputs = 17\n",
    "        # pitch nodes = 12, timbre clusters = 3, rhythm = 1, loudness=1 ... add together\n",
    "        for i in range(n_inputs):\n",
    "            # create one node per input feature\n",
    "            x = random.randint(50, width-50)\n",
    "            # choose a random integer for x positioning between 2 values a,b - 50 is arbitrary, width-50 to ensure it stays wtihin window\n",
    "            y = random.randint(50, height//3)\n",
    "            # also randomized for y coordinate, 50 is still arbitrary but consistent iwth top, height//3 (integer division, keep it simple)\n",
    "            # is so that all nodes are\n",
    "            # in top third of the screen for the first layer\n",
    "            self.input_nodes.append(Node(x,y,\"input\"))\n",
    "            # appends and adds this node to the input nodes at the x,y coordinate randomized above and labels it as input\n",
    "\n",
    "        association_inputs = 8\n",
    "        # associations to draw from raw feature data \n",
    "        # for instance: Pitch + Timbre -> color // Pitch + Rhythm -> movement //Loudness + Timbre -> shape size // Timbre cluster -> shape type\n",
    "        # just different varied combinations of raw features to the association made with it in a person with chromestesia mind\n",
    "        # more associations means more complex\n",
    "        for i in range(association_inputs):\n",
    "            # loop through association input amount\n",
    "            x = random.randint(100, width-100)\n",
    "            # same as before, randomize x position \n",
    "            # bigger padding since the nodes from above will converge onto less nodes here\n",
    "            y = random.randint(height//3, height//2)\n",
    "            # again, randomize y position by taking the middle portion of the display (integer dvision of the three parts to get top third,\n",
    "            # and then get bottom part so y must be in middle layer)\n",
    "            # contribute to downward vertical flow\n",
    "            self.association_nodes.append(Node(x,y,\"association\"))\n",
    "            # add the node from this iteration into the association nodes with randomized position and association label\n",
    "\n",
    "        output_layer = 12\n",
    "        # 12 outputs- correspond to visual elements users will see (color, shape) (again, higher value, means more complex)\n",
    "        for i in range(output_layer):\n",
    "            # loop through output layer amount\n",
    "            x = random.randint(50,width-50)\n",
    "            # again, randomly set x coordinate\n",
    "            # made the value 50 again since the output layer has more nodes than middle layer, richer\n",
    "            y = random.randint(height//2, height-50)\n",
    "            # randomize y coordinate, bottom part of display through integer divison and padding so it doesn't pass edge of display\n",
    "            self.output_nodes.append(Node(x,y,\"output\"))\n",
    "            # append node to output nodes with randomized location and output label\n",
    "\n",
    "        for inputs in self.input_nodes:\n",
    "            # loop through each input layer node\n",
    "            for association in self.association_nodes:\n",
    "                # then loop through each association node\n",
    "                inputs.add_edge(association, weight=random.uniform(0.02,0.5))\n",
    "                # add an edge between input and association nodes\n",
    "                # for now, random weights to simulate how each node is not connected with same strength in our human mind\n",
    "        for association in self.association_nodes:\n",
    "            # loop through association layer nodes\n",
    "            for output in self.output_nodes:\n",
    "                # then loop through each output node\n",
    "                association.add_edge(output, weight=random.uniform(0.02,0.5))\n",
    "                # again add an edge, connect association to output layer with randomly assigned weights\n",
    "    \n",
    "    # next two functions also explained in detail in video output file- just relating to starting/stopping play\n",
    "    def start(self):\n",
    "        \"\"\"Start visualization and play audio.\"\"\"\n",
    "        \n",
    "        # load and play audio\n",
    "        pygame.mixer.music.load(self.audio_path)\n",
    "        self.start_time = time.time()\n",
    "        pygame.mixer.music.play()\n",
    "        self.running = True\n",
    "        self.current_frame = 0\n",
    "\n",
    "        # while the visualization is in action\n",
    "        while self.running and self.current_frame < len(self.times):\n",
    "            self.stop()\n",
    "            self.update()\n",
    "            self.draw()\n",
    "\n",
    "            if not pygame.mixer.music.get_busy(): # if the audio is no longer playing\n",
    "                self.running = False\n",
    "\n",
    "            pygame.display.flip() # make changes appear on display\n",
    "            self.clock.tick(self.fps)\n",
    "\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "    def stop(self):\n",
    "        \"\"\"Handle quit events.\"\"\"\n",
    "        for event in pygame.event.get(): # for anything event object \n",
    "            if event.type == pygame.QUIT: # if event is of type QUIT (eg. close window)\n",
    "                self.running = False\n",
    "                pygame.mixer.music.stop() \n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\" \n",
    "        Update node and edge activation based on audio\n",
    "        \"\"\"\n",
    "        current_time = time.time()-self.start_time\n",
    "        # calculate current time- take time right now and subtract by start time (0:0)\n",
    "        while self.current_frame < len(self.times) and current_time >= self.times[self.current_frame]:\n",
    "            # loop through while the current frame index is less than the number of total frames and \n",
    "            # the current time that the video has played for is more than  the current time frame --> meant to ensure current frame has been triggered\n",
    "            pitch_node_idx = int(self.midi[self.current_frame]*12)\n",
    "            # map midi note of current frame to an input node index, hence the multiply by 12 to spread across nodes\n",
    "            # self.midi is the array of midi numbers extracted from features, indexing at current frame to give MIDI at this time\n",
    "            # turned to int to prevent float error\n",
    "            self.input_nodes[pitch_node_idx % len(self.input_nodes)].activation = self.rms[self.current_frame]\n",
    "            # set activation of input to be the current frame's rms\n",
    "            # indexes the input nodes to  be the pitch node (wrap around if exceeds input noes through the remainder of length of input nodes)\n",
    "            # does this by taking pitch nodee (MIDI note integer index, find remainder of that divided by all input nodes in case it exceeds total- avoid idnex error)\n",
    "            cluster_idx = self.cluster_labels[self.current_frame]\n",
    "            # finding cluster index by takingthe cluster labels (from k-means clustering of mfcc- which is 0 1 2) based on current frame\n",
    "            self.input_nodes[12 + cluster_idx].activation = self.rms[self.current_frame]\n",
    "            # indexing input nodes, 12 represent the pitch (midi notes) and the cluster index is based on the three timbre options, so \n",
    "            # this is indexing one of the timbre nodes and activating it based on current frame rms\n",
    "            self.input_nodes[-1].activation = self.rms[self.current_frame]\n",
    "            # last node in input layer is for loudness\n",
    "            # set to be activated based on the current frame's loudness\n",
    "            self.current_frame = self.current_frame + 1\n",
    "            # then increase the current frame by one for next loop\n",
    "            \n",
    "        for node in self.input_nodes + self.association_nodes:\n",
    "            # for all the nodes in input layer\n",
    "            for edge in node.edges:\n",
    "                # for all the edges for the specific node\n",
    "                edge.end_node.activation = edge.end_node.activation + node.activation * edge.weight * 3.0 \n",
    "                # the end node is activated by adding that node's activation by current node activation multiplied by the weight of the edge (scaled by 1.5)\n",
    "                edge.end_node.activation = min(edge.end_node.activation, 1.0) \n",
    "                # the end node activation capped at 1\n",
    "        \n",
    "        for node in self.input_nodes + self.association_nodes + self.output_nodes:\n",
    "            # for every node in all three layers\n",
    "            node.activation = node.activation*0.85\n",
    "            # the node activation is reduced slightly each frame (fading out) by multipling current activation by 0.85- smoother\n",
    "\n",
    "    def draw(self):\n",
    "        # draw on pygame display so user has visual output\n",
    "        self.screen.fill((0,0,0))\n",
    "        #clear display to black at the start of the frame\n",
    "        layer_colors = { \"input\": (0,128, 255), \"association\":(255, 128, 0), \"output\": (128,255,0) }\n",
    "        # layer base colors, input is blue, association is orange, output is green        \n",
    "        \n",
    "        for node in self.input_nodes + self.association_nodes:\n",
    "            # for node in input and association node displays\n",
    "            for edge in node.edges:\n",
    "                # all edges for current node\n",
    "                color_intensity = (edge.weight*200) \n",
    "                # set color intensity based on edge weight but make the max 200 (not 255 to prevent white from taking over)\n",
    "                color_intensity = max(0,min(255,color_intensity))\n",
    "                #set color intensity to be the set color intensity, unless it exceeds 255 then max it out there\n",
    "                if node.type == \"input\":\n",
    "                    # if the node is in the input layer\n",
    "                    edge_color = (0,0,color_intensity)\n",
    "                    # change the intensity of the blue part of rgb -> shades of blue\n",
    "                else:\n",
    "                    # only other layer looped through is assoication here\n",
    "                    edge_color = (color_intensity, 128,0)\n",
    "                    # alter edge color based on color intensity of red -> shades of orange\n",
    "                pygame.draw.line(self.screen, edge_color, (int(edge.start_node.x), int(edge.start_node.y)),(int(edge.end_node.x), int(edge.end_node.y)), max(1, int(edge.weight*3)))\n",
    "                # self.screen is where we draw the line\n",
    "                # alter red and green color intensities based on what they are set as before for rgb, but keep blue tint the same for consistency\n",
    "                # start drawing at the current start node coordinates and finish drawing line at end note coordinates\n",
    "                # width is set at end to ensure each line is at least 1 pixel, but really trying to set it at the weight of the edge (multiply by 3)\n",
    "        for node in self.input_nodes + self.association_nodes + self.output_nodes:\n",
    "            #iterate over all nodes including output nodes now\n",
    "            radius= max(1,min(self.max_radius, int(self.max_radius * node.activation +5)))\n",
    "            # choose circle radius, 1 is the minimum radius (make sure it shows up), then it can either be the max radius (set above) or the integer of\n",
    "            # the self radius * node activation + 5 (to ensure low activatioin ones still noticeable) to increase it\n",
    "            intensity = max(50, min(255, int(255*node.activation)))\n",
    "            # visibility/intensity of color\n",
    "            if node.type == \"input\":\n",
    "                color = (0,0,intensity)\n",
    "                # shades of blue altered in rgb\n",
    "            if node.type == \"association\":\n",
    "                color = (intensity,128,0)\n",
    "                # shades of orange altered by altering red in rgb\n",
    "            if node.type == \"output\":\n",
    "                color = (128,intensity, 0)\n",
    "                # shades of green altered in rgb\n",
    "            if node.type == \"input\":\n",
    "                pygame.draw.circle(self.screen,color,(int(node.x),int(node.y)), radius)\n",
    "            elif node.type == \"association\":\n",
    "                size = radius * 2\n",
    "                pygame.draw.rect(self.screen,color,(int(node.x - radius), int(node.y - radius), size, size))\n",
    "            elif node.type == \"output\":\n",
    "                pygame.draw.circle(self.screen,color,(int(node.x),int(node.y)), radius)\n",
    "\n",
    "# test case\n",
    "audio_path = \"the-best-jazz-club-in-new-orleans-164472.mp3\"\n",
    "# audio set to be jazz music\n",
    "features = extract_audio_features(audio_path, duration=30)\n",
    "# extract features\n",
    "product = NeuralVisualizer(audio_path, features)\n",
    "# use the neural visualizer!\n",
    "product.start()\n",
    "# start\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b526b-3f5c-4de8-845e-9b78d66b91e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
