{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8787abcb-96b6-4330-90a2-11e4eafd8027",
   "metadata": {},
   "source": [
    "# Network Model 2\n",
    "This visualization is meant to mimic a human neural network. This model consists of three layers: input layer, association layer, and output layer. The input layer intakes the raw feature data such as loudness, pitch, timbre from the audio. The association layer are the intermediate chromesthesia associations between input features and visual features - mostly working to combine multiple audio features to then activate a visual image associated with those features. Output layer nodes are the visual layer in which activation determines what shapes, color , intnesity the user sees. As the visual output begins, all nodes in adjacent layers are automatically connected to each other (input nodes all connected to association, association nodes all connected to output) with randomized weights set, but as the audio plays the weights have the opportunity to be strengthened based on the learning mechanisms in play. When the audio plays, the input nodes are set, they are turned on in proportion to its strength in the audio. They now contain a signal which is then transmitted to following layers through the edges, similating how action potentials are propagated through neurons carrying a signal. If both the start and end node are activated at the same time, the edge weight increases (Hebbian learning- \"nodes that fire together, wire together), hence why edges grow thicker\n",
    "\n",
    "Sources:\n",
    "- https://www.jeremykun.com/2012/12/09/neural-networks-and-backpropagation/?utm_source=chatgpt.com- setting up nodes and edges\n",
    "- https://github.com/SophieWalden/snakeNeuralNetwork/blob/master/snakeNN.py- neural network example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0826b73f-4eb8-4dde-826c-0b8884a1bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411029d4-35b6-4ce8-9418-4d08a2e94468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pygame\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea9d846-7efd-4fca-9b9e-3740ef22623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented same way as from feature extraction doc\n",
    "\n",
    "# predefined colors for each note (implement user entry later)\n",
    "NOTE_RGB = {\n",
    "    \"C\":  (214, 174, 16),\n",
    "    \"C#\": (115, 58, 75),\n",
    "    \"D\":  (38, 63, 145),\n",
    "    \"D#\": (68, 118, 67),\n",
    "    \"E\":  (211, 87, 49),\n",
    "    \"F\":  (159, 194, 76),\n",
    "    \"F#\": (195, 10, 103),\n",
    "    \"G\":  (255, 156, 223),\n",
    "    \"G#\": (37, 149, 150),\n",
    "    \"A\":  (155, 152, 223),\n",
    "    \"A#\": (10, 107, 62),\n",
    "    \"B\":  (238, 145, 50)\n",
    "}\n",
    "NOTE_NAMES = [\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"]\n",
    "\n",
    "\n",
    "def extract_audio_features(audio_path: str, duration: float = None, HOP_LENGTH: int = 2048, FRAME_LENGTH: int = 2048 ):\n",
    "    \"\"\"\n",
    "    This function loads an audio file and extracts audio features: pitch (f0), loudness (RMS energy), and timbre (MFCC) for each beat frame.\n",
    "    It also normalizes rms and MFCC on a 0-1 scale for visual mapping.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path: str\n",
    "            path to any audio file format (.wav, .mp3, .ogg, etc.)\n",
    "        duration: float, optional\n",
    "            Duration from the start of the file (seconds). \n",
    "            Loads the full track if none. \n",
    "        hop_length: int, fixed\n",
    "            Number of samples between frames.\n",
    "        frame_length: int, fixed\n",
    "            Number of samples per frame. \n",
    "\n",
    "    Returns:\n",
    "         audio_features : dict\n",
    "            times: array of frame timestamps,\n",
    "            midi: midi number ,\n",
    "            rms: normalized loudness,\n",
    "            mfcc: normalized MFCC matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # load audio file\n",
    "    y, sr = librosa.load(audio_path, duration=duration)\n",
    "\n",
    "    # pitch (f0) extraction\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y,\n",
    "                                             sr=sr,\n",
    "                                             frame_length = FRAME_LENGTH,\n",
    "                                             hop_length = HOP_LENGTH,\n",
    "                                             fmin=librosa.note_to_hz('A0'),\n",
    "                                             fmax=librosa.note_to_hz('C8'))\n",
    "    f0 = np.nan_to_num(f0, nan=np.nanmean(f0))  \n",
    "    # if there are NaN values, replace them with the mean pitch\n",
    "    midi = librosa.hz_to_midi(f0)\n",
    "    # convert frequency to midi note number\n",
    "\n",
    "    # loudness(RMS energy) extraction\n",
    "    rms = librosa.feature.rms(y=y,frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0]\n",
    "\n",
    "    # time stamps for mapping to pygame \n",
    "    times = librosa.times_like(rms, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # normalize numerical features for mapping (0â€“1 range)\n",
    "    def normalize(x):\n",
    "        min = np.min(x)\n",
    "        max = np.max(x)\n",
    "        denom = max - min\n",
    "        if denom == 0:\n",
    "            return np.zeros_like(x)  \n",
    "        return (x - min) / denom\n",
    "    \n",
    "    rms_norm = normalize(rms)\n",
    "\n",
    "    # dictionary of audio features for the funciton to return\n",
    "    audio_features = {\n",
    "       \"times\" : times,\n",
    "       \"midi\" : midi,\n",
    "       \"rms\" : rms_norm,\n",
    "       \"sr\" : sr \n",
    "       }\n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c64b401-1b38-4643-9f14-046b38db626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # creates class for nodes\n",
    "    def __init__(self, x, y, node_type=\"input\", feature=None):\n",
    "        # init function, takes in self, x, y (referring to coordinate positions in x and y planes), what layer the node is a part of\n",
    "        self.x = x\n",
    "        # node's x position\n",
    "        self.y = y\n",
    "        # refers to node's y position\n",
    "        self.type = node_type\n",
    "        # refers to what layer the node is a part of- this neural network will have three layers (input, association, output/visual), helps for display\n",
    "        self.feature = feature\n",
    "        # pitch, rms, timbre --> what each node will represent\n",
    "        self.activation = 0.0\n",
    "        # current activation level for node\n",
    "        self.pulse_size = 1.0\n",
    "        # how much the node will pulse\n",
    "        self.edges = []\n",
    "        # initialize empty list to hold edges\n",
    "        self.glow = 0.0\n",
    "        # initialize to 0\n",
    "    def add_edge(self, target_node, weight= 0.1):\n",
    "        self.edges.append(Edge(self,target_node,weight))\n",
    "        # actually able to add the edge between noe\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, start_node, end_node, weight= 0.1):\n",
    "        \"\"\"\n",
    "        sends activation from starting node to target node. \n",
    "        \"\"\"\n",
    "        # this function describes how to add edges\n",
    "        self.start_node = start_node\n",
    "        # sets start node as where signal begins\n",
    "        self.end_node = end_node\n",
    "        # sets end node, where edge will be connected to/end\n",
    "        self.weight = weight\n",
    "        # strength of edge/connection between nodes with a higher value meaning more connection (this weight will be modified)\n",
    "    def propagate(self):\n",
    "        signal = self.start_node.activation * self.weight\n",
    "        # self.from_node.activation = node we are starting at, activation is coming from audio and its extracted features\n",
    "        # weight refers to strength of connective edges between nodes\n",
    "        # multiplying them together to get the size of signal - if activation and weight are both high, then there's a big signal\n",
    "        # if weight is low, then there's a weak signal, but if the node isn't activated at all (or weak) then no signal sent (or just super weak)\n",
    "        self.end_node.activation = self.end_node.activation + signal\n",
    "        # adds the already existent activity in the ending node of the edge and adds the signal propagating through to it\n",
    "\n",
    "def coloradjust(rgb, factor):\n",
    "        \"\"\"\n",
    "        Darken or lighten RGB based on the octave\n",
    "        \"\"\"\n",
    "        rgb = np.array(rgb, dtype=float)\n",
    "        # convert rgb value to array so that we can alter, floats for precision\n",
    "        newrgb = rgb * factor\n",
    "        # multiples rgb by the factor associated with the octave\n",
    "        newrgb[newrgb < 0] =0\n",
    "        # if the new calculated rgb is below 0, just make it 0 so that it is within the number range for rgb\n",
    "        newrgb[newrgb > 255] = 255\n",
    "        # similarly, if above 255 just make it 255 to make sure it isn't above number range for rgb\n",
    "        return tuple(newrgb.astype(int))\n",
    "        # returns a tuple / by 255 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4111d192-7f47-4497-a032-645292e1081a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class NeuralVisualizer:\n",
    "    \"\"\"\n",
    "    Create neural visualizer class that will create a visualization of the neural network output. \n",
    "    Consists of three layers, input, association, output, each with separate nodes. Meant to simulate a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self,audio_path,features,height=800,width=1200, fps=60, learning_rate=0.01):\n",
    "        # init function, takes in audio path to connect to audio, features as created above (shapes, colors, etc), size of image, frames per second, and learning rate\n",
    "        pygame.init()\n",
    "        # initialize all pygame parts\n",
    "        pygame.mixer.init()\n",
    "        # initialize audio playback\n",
    "        pygame.display.set_caption(\"Chromesthesia Neural Visualizer\")\n",
    "        # captions with informative title\n",
    "        self.clock = pygame.time.Clock()\n",
    "        # control frame rate and time\n",
    "        self.learning_rate = learning_rate\n",
    "        # used for learnign rate of node associations - will change over time\n",
    "        self.width = width \n",
    "        # set width display\n",
    "        self.height = height\n",
    "        # set height display\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        # this creates the display/window for output and sets it to width/height set above\n",
    "        self.fps = fps\n",
    "        # frames per second- controls how often this is updated in the window\n",
    "\n",
    "        self.start_time = 0\n",
    "        # when playback starts for audio and visualization\n",
    "        self.current_frame = 0\n",
    "        # tracks frames of visualization, intialize to zero to start\n",
    "        self.running = False\n",
    "        # don't start running yet\n",
    "\n",
    "        self.audio_path = audio_path\n",
    "        # connects to audio\n",
    "        self.times = features[\"times\"]\n",
    "        # timestamps for each frame\n",
    "        self.midi = features[\"midi\"]\n",
    "        # takes midi notes- pitch in midi numbers associated with color\n",
    "        self.rms = features[\"rms\"]\n",
    "        # loudness- used for scaling\n",
    "        self.sr = features[\"sr\"]\n",
    "        # sample rate of audio\n",
    "        \n",
    "\n",
    "        self.input_nodes = []\n",
    "        # input layer nodes- represents raw features of audio-  in this case the note\n",
    "        for i,note in enumerate(NOTE_NAMES):\n",
    "            x= 100+ i*120\n",
    "            # horizontal spacing, between each node (played around with numbers, kind of random)\n",
    "            y= 100\n",
    "            # vertical spacing \n",
    "            self.input_nodes.append(Node(x,y, \"input\", note))\n",
    "            # appends empty list with the node x/y position, input label, and note associated with it\n",
    "\n",
    "        octaves = [1,2,3,4,5,6,7]\n",
    "        self.association_nodes = []\n",
    "        # association/middle layer of nodes- create the chromestesia associations- notes to octaves\n",
    "        for i,octave in enumerate(octaves):\n",
    "            x= 150+ i*120\n",
    "            # again horizontal position\n",
    "            y= 300\n",
    "            # again veritcal\n",
    "            self.association_nodes.append(Node(x,y, \"association\", octave))\n",
    "            # again append positon, label, and octave to association\n",
    "            \n",
    "        self.output_nodes = []\n",
    "        # output/visualization layer- visual representation of nodes\n",
    "        for i,note in enumerate(NOTE_NAMES):\n",
    "            x= 100+ i*90\n",
    "            # again horizontal position\n",
    "            y= 500\n",
    "            # again vertical\n",
    "            self.output_nodes.append(Node(x,y, \"output\", note))\n",
    "            # append position, label, octave again\n",
    "        self.max_radius = 20\n",
    "        # just sets a maximum size a node can grow to\n",
    "\n",
    "        for in_node in self.input_nodes:\n",
    "            for association_node in self.association_nodes:\n",
    "                in_node.add_edge(association_node, weight = random.uniform(0.1,0.3))\n",
    "                # creates an edge from input node to association node with a random weight associated\n",
    "                # edges represent strength of relationships between notes and octaves\n",
    "        for association_node in self.association_nodes:\n",
    "            for out_node in self.output_nodes:\n",
    "                association_node.add_edge(out_node, weight=random.uniform(0.1,0.3))\n",
    "                # this one is association node to output node edges with again random weight associated to start with\n",
    "\n",
    "    \n",
    "    # next two functions also explained in detail in video output file- just relating to starting/stopping play\n",
    "    def start(self):\n",
    "        \"\"\"Start visualization and play audio.\"\"\"\n",
    "        \n",
    "        # load and play audio\n",
    "        pygame.mixer.music.load(self.audio_path)\n",
    "        self.start_time = time.time()\n",
    "        pygame.mixer.music.play()\n",
    "        self.running = True\n",
    "        self.current_frame = 0\n",
    "\n",
    "        # while the visualization is in action\n",
    "        while self.running and self.current_frame < len(self.times):\n",
    "            self.stop()\n",
    "            self.update()\n",
    "            self.draw()\n",
    "\n",
    "            if not pygame.mixer.music.get_busy(): # if the audio is no longer playing\n",
    "                self.running = False\n",
    "\n",
    "            pygame.display.flip() # make changes appear on display\n",
    "            self.clock.tick(self.fps)\n",
    "\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "    def stop(self):\n",
    "        \"\"\"Handle quit events.\"\"\"\n",
    "        for event in pygame.event.get(): # for anything event object \n",
    "            if event.type == pygame.QUIT: # if event is of type QUIT (eg. close window)\n",
    "                self.running = False\n",
    "                pygame.mixer.music.stop() \n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\" \n",
    "        Update node and edge activation based on audio\n",
    "        \"\"\"\n",
    "        current_time = time.time()-self.start_time\n",
    "        # calculate current time- take time right now and subtract by start time (0:0)\n",
    "        while self.current_frame < len(self.times) and current_time >= self.times[self.current_frame]:\n",
    "            # loop through while the current frame index is less than the number of total frames and \n",
    "            # the current time that the video has played for is more than  the current time frame --> meant to ensure current frame has been triggered\n",
    "            midi_val = int(round(self.midi[self.current_frame]))\n",
    "            #convert midi value for this specific frame (indexed) , rounded to an int\n",
    "            note_name = NOTE_NAMES[midi_val % 12]\n",
    "            # associates midi value with note name by dividing midi value by 12 (number of notes)\n",
    "            octave_val = midi_val // 12\n",
    "            # get octave number, divide(integer division) midi value by 12 (so then MIDI 60 --> 5 octave value... how midi notes are mapped)\n",
    "            for node in self.input_nodes:\n",
    "                # activates the input node that matches the feature, if not associated feature then everything else is at base 0 activation\n",
    "                node.activation = 1.0 if node.feature == note_name else 0.0\n",
    "            octave_val = int(self.midi[self.current_frame] // 12)\n",
    "            # recalculate octave value ofor current frame and ensure it's an int\n",
    "            for node in self.association_nodes:\n",
    "                # for the node in association layer, activated if the node feature is equivalent to the octave value, otherwise again 0 activation\n",
    "                node.activation = 1.0 if node.feature == octave_val else 0.0\n",
    "            self.current_frame = self.current_frame + 1\n",
    "            # increase current frame by one\n",
    "            \n",
    "        for node in self.input_nodes + self.association_nodes:\n",
    "            # for all the nodes in input layer\n",
    "            for edge in node.edges:\n",
    "                # for all the outgoing edges for the specific node, propagate activation forward to next node\n",
    "                edge.propagate()\n",
    "        \n",
    "        for node in self.input_nodes + self.association_nodes + self.output_nodes:\n",
    "            # for every node in all three layers\n",
    "            node.activation = node.activation*0.85\n",
    "            # node activation is reduced for each frame by 0.85 factor\n",
    "\n",
    "        current_frame_index = max(0,self.current_frame-1)\n",
    "        # use frame index one prior, but never less than 0\n",
    "        for node in self.output_nodes:\n",
    "            # for each node in output layer\n",
    "            if node.feature == NOTE_NAMES[int(round(self.midi[current_frame_index])) %12]:\n",
    "                # if the node feature is the same as note names list indexed at the midi index of the current frame divided by 12 \n",
    "                node.activation = self.rms[current_frame_index]\n",
    "                # then node activation is the self rms value for the current frame\n",
    "                # activation based on loudness // controls pulsing strength\n",
    "\n",
    "    def draw(self):\n",
    "        # draw on pygame display so user has visual output\n",
    "        self.screen.fill((0,0,0))\n",
    "\n",
    "        font = pygame.font.SysFont(\"Arial\", 15, bold=True)\n",
    "        # set font to be arial\n",
    "        input_label = font.render(\"Input Layer (Notes-MIDI)\", True, (200,200, 255))\n",
    "        association_label = font.render(\"Association Layer (Note-Octaves)\", True, (200,200, 255))\n",
    "        output_label = font.render(\"Output Layer (Visualization)\", True, (200,200, 255))\n",
    "        self.screen.blit(input_label, (self.width//2 - input_label.get_width()//2, 40))\n",
    "        self.screen.blit(association_label, (self.width//2 + 400 - input_label.get_width()//2, 300))\n",
    "        self.screen.blit(output_label, (self.width//2 - input_label.get_width()//2, 650))\n",
    "\n",
    "\n",
    "\n",
    "        for node in self.input_nodes + self.association_nodes:\n",
    "            # loop through each node in input/association layers\n",
    "            for edge in node.edges:\n",
    "                # each outgoing edge for the node\n",
    "                color_intensity = int(edge.weight * 200)\n",
    "                # edge weight converted to intensity value for color (multiply by 200, within 0-255 range for rgb value)\n",
    "                # controls brightness of eddge\n",
    "                color_intensity = max(0,min(255, color_intensity))\n",
    "                # # ensures doesn't exceed 255\n",
    "                edge_color = (0,0, color_intensity)\n",
    "                # edge color set to be the blue color intensity value \n",
    "                pygame.draw.line(self.screen, edge_color, (int(edge.start_node.x), int(edge.start_node.y)), (int(edge.end_node.x), int(edge.end_node.y)), max(1, int(edge.weight*8)))\n",
    "                # draw the edge with created edge color starting at x,y coordinate of start node and ending at x,y coordinate of end node, with edge weight of either 1 as the minimum o the edge weight scaled to times 8 for max\n",
    "\n",
    "        for node in self.input_nodes + self.association_nodes:\n",
    "            base_radius = 5\n",
    "            # if node is inactive then radius is 5\n",
    "            radius = base_radius + int(node.activation*10)\n",
    "            # radius changed to base radius plus the node activation value multipled by ten (int value to avoid error)\n",
    "            color = (0,0,245)\n",
    "            # color set to be blue for all of them\n",
    "            pygame.draw.circle(self.screen,color, (int(node.x), int(node.y)), radius)\n",
    "            # draw a circle for each node for input/association nodes for this x,y coordinate and radius\n",
    "\n",
    "        \n",
    "        for node in self.output_nodes:\n",
    "            # for output nodes, glow reset to 0 each time\n",
    "            node.glow = 0\n",
    "        for association_node in self.association_nodes:\n",
    "            if association_node.activation > 0:\n",
    "                for edge in association_node.edges:\n",
    "                    # but if association node activation is more than 0, then the edge will glow to either the edge glow (base 0 and the association node activation value added together) or just 1\n",
    "                    edge.end_node.glow = min(1.0, edge.end_node.glow + association_node.activation)\n",
    "        for node in self.output_nodes:\n",
    "            # then for node glow, it needs to decrease/decay for smooth transitions, multiply by .85 factor\n",
    "            node.glow = node.glow * 0.95\n",
    "                    \n",
    "        \n",
    "        for node in self.input_nodes + self.association_nodes + self.output_nodes:\n",
    "            # for nodes in all layers\n",
    "            if node.type == \"input\" or node.type == \"association\":\n",
    "                # if node is input or association labelled\n",
    "                radius = 8 if node.activation > 0 else 5\n",
    "                # if node is activated radius is 8 otherwise it stays 5 (on/off pulsing with audio)\n",
    "                color = (0,0,245)\n",
    "                # color set to a blue value\n",
    "                pygame.draw.circle(self.screen,color,(int(node.x), int(node.y)), radius)\n",
    "                # draw nodes at x/y coordinate with set radius\n",
    "            elif node.type == \"output\":\n",
    "                # but if output labelled\n",
    "                radius = max(8, int(8+node.activation*15))\n",
    "                # then radius changes based on node activation value\n",
    "                color = NOTE_RGB[node.feature]\n",
    "                # color is based on the color mapping from up above\n",
    "                factor = 1 + node.glow\n",
    "                # factor is 1 plus the node glow value \n",
    "                color = coloradjust(color, factor)\n",
    "                # then use color adjust function\n",
    "                pygame.draw.circle(self.screen, color, (int(node.x), int(node.y)), radius)\n",
    "                # draw circle with set color and coordinate and radius\n",
    "\n",
    "        \n",
    "\n",
    "# test case\n",
    "audio_path = \"emotional-piano-005-am-80-97777.mp3\"\n",
    "# audio set to be simple piano chords\n",
    "features = extract_audio_features(audio_path, duration=30)\n",
    "# extract features\n",
    "product = NeuralVisualizer(audio_path, features)\n",
    "# use the neural visualizer!\n",
    "product.start()\n",
    "# start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b526b-3f5c-4de8-845e-9b78d66b91e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
